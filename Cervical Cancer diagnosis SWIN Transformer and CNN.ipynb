{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610716e6-eddd-4818-9b8e-d261222b4a21",
   "metadata": {},
   "source": [
    "# Analysis of Colposcopic Images Obtained from \"International Agency for Research on Cancer\" (IARCImageBankColpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269cf10-2b9d-45b7-965e-d39af6ea86ad",
   "metadata": {},
   "source": [
    "### Refined Criteria Summary for Grouping the 200 Cases into Normal, Cancerous, Precancerous and Inconclusive\n",
    "\n",
    "#### 1. **Normal Group**\n",
    "**Criteria**:\n",
    "- **HPV Status**: Both positive and negative.\n",
    "- **Sample Adequacy**: Adequate colposcopic sample.\n",
    "- **Squamocolumnar Junction Visibility**: Completely visible or not visible.\n",
    "- **Transformation Zone**: Type 1, Type 2, or Type 3.\n",
    "- **Normal Colposcopic Findings**:\n",
    "  - Presence of original squamous epithelium, columnar epithelium, or metaplastic squamous epithelium.\n",
    "  - Absence of significant abnormal colposcopic findings (e.g., no dense acetowhite epithelium, coarse punctation, or mosaic).\n",
    "  - Iodine staining: Nil or transparent, faintly or patchy yellow, or brown.\n",
    "- **Swede Score**: Low scores (0-2).\n",
    "- **Histopathology**: Normal findings (e.g., mature squamous epithelium, atrophic changes, nabothian cysts).\n",
    "- **Management**: Routine screening after 5 years or repeat HPV test/colposcopy after 1 year if HPV positive.\n",
    "\n",
    "#### 2. **Pre-cancerous Group**\n",
    "**Low-grade Squamous Intraepithelial Lesion (LSIL)**\n",
    "**Criteria**:\n",
    "- **HPV Status**: Both positive and negative.\n",
    "- **Sample Adequacy**: Adequate colposcopic sample.\n",
    "- **Squamocolumnar Junction Visibility**: Completely visible.\n",
    "- **Transformation Zone**: Type 1 or Type 2.\n",
    "- **Colposcopic Findings**:\n",
    "  - Thin acetowhite epithelium.\n",
    "  - Irregular borders, fine mosaic, fine punctation.\n",
    "  - Mild abnormalities without features of high-grade lesions.\n",
    "- **Swede Score**: Moderate scores (3-4).\n",
    "- **Iodine Uptake**: Faintly or patchy yellow, or nil or transparent.\n",
    "- **Histopathology**: Low-grade lesions (CIN1).\n",
    "- **Management**: Routine follow-up and colposcopy, with biopsies as indicated.\n",
    "\n",
    "**High-grade Squamous Intraepithelial Lesion (HSIL)**\n",
    "**Criteria**:\n",
    "- **HPV Status**: Both positive and negative.\n",
    "- **Sample Adequacy**: Adequate colposcopic sample.\n",
    "- **Squamocolumnar Junction Visibility**: Completely or partially visible.\n",
    "- **Transformation Zone**: Type 1, Type 2, or Type 3.\n",
    "- **Colposcopic Findings**:\n",
    "  - Dense acetowhite epithelium, coarse mosaic, or coarse punctation.\n",
    "  - Sharp borders, ridge sign, inner border sign.\n",
    "  - Cuffed crypt (gland) openings, rapid appearance of acetowhite, presence of atypical vessels.\n",
    "- **Swede Score**: High scores (5-10).\n",
    "- **Iodine Uptake**: Distinctly yellow, indicating non-staining with iodine.\n",
    "- **Histopathology**: High-grade lesions (CIN2, CIN3).\n",
    "- **Management**: Treatment options like LLETZ, punch biopsies, or excisional procedures.\n",
    "\n",
    "#### 3. **Cancerous Group**\n",
    "**Criteria**:\n",
    "- **HPV Status**: Positive.\n",
    "- **Sample Adequacy**: Adequate colposcopic sample.\n",
    "- **Squamocolumnar Junction Visibility**: Not visible.\n",
    "- **Transformation Zone**: Type 3.\n",
    "- **Colposcopic Findings**:\n",
    "  - Dense acetowhite epithelium, coarse punctation, or mosaic.\n",
    "  - Sharp borders, ridge sign, inner border sign, presence of atypical vessels.\n",
    "  - Features suspicious for invasion: irregular surface, erosion, tumor, or gross neoplasm.\n",
    "- **Swede Score**: High scores (9-10).\n",
    "- **Iodine Uptake**: Distinctly yellow or non-staining with iodine.\n",
    "- **Histopathology**: Invasive cancer (e.g., squamous cell carcinoma, adenocarcinoma).\n",
    "- **Management**: Surgical intervention (e.g., LLETZ, punch biopsies, multiple biopsies).\n",
    "\n",
    "#### 4. **Inconclusive Group**\n",
    "**Criteria**:\n",
    "- **HPV Status**: Both positive and negative.\n",
    "- **Sample Adequacy**: Inadequate colposcopic sample due to reasons such as:\n",
    "  - Extensive inflammation.\n",
    "  - Cervix not satisfactorily exposed.\n",
    "  - Stenosis of vagina or atrophy of cervix.\n",
    "- **Squamocolumnar Junction Visibility**: Not visible.\n",
    "- **Transformation Zone**: Not applicable.\n",
    "- **Colposcopic Findings**: Not applicable due to inadequate sample.\n",
    "- **Swede Score**: Not applicable.\n",
    "- **Histopathology**: Not done or not conclusive.\n",
    "- **Management**: Further investigation, control of infection, or clinical radiological findings based follow-up.\n",
    "\n",
    "This refined summary ensures that all criteria used for categorization are thoroughly examined and accurately represented. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e47b11-fc12-412f-bd9c-221fd8c2b444",
   "metadata": {},
   "source": [
    "**Given the above criteria, Colposcopy images were grouped into four sets (based on the colposcopy examination and histopathology findings)**\n",
    "    \n",
    "\n",
    "1.\tPrecancerous (77 unique subjects\n",
    "   )\n",
    "2.\tCancer (26 unique subjects)\n",
    " \n",
    "3.\tInconclusive (3 unique subjects) – these were excluded to ensure that the comparison between cancerous and normal conditions remains clear and unambiguou\n",
    "s.\n",
    "4.\tNormal (94 unique subjects) – these are labelled normal from Colposcopy examination and/or histopathology outcomes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad78b0-22bf-4c06-b7d2-720d7ef374fc",
   "metadata": {},
   "source": [
    "**Steps to process the cervical colposcopic images organized by case numbers and prepare them for SWIN-Transformer CNN analysis using Python. To prepare images for SWIN-Transformer CNN analysis, organize and extract the images from their respective case folders. Here is a step-by-step guide on how to achieve this using Python:**\n",
    "\n",
    "    Organize the Directory Structure such that each case folder (e.g., \"Case 001\", \"Case 002\") contains its respective images. (done)\n",
    "\n",
    "    Extract Images and Prepare for Analysis: Write a Python script to extract images from each folder and prepare them for input into the SWIN-Transformer CNN.\n",
    "\n",
    "    Install Necessary Python Libraries installed e.g. os, shutil, glob, and PIL for handling file operations and image processing.\n",
    "\n",
    "    Script to Extract Images and Prepare Data given next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c5deed-80f3-45eb-9c02-8e873e620871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7327716-2881-4a96-8762-fc558d5d6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the metadata CSV file and create a diagnosis dictionary\n",
    "metadata_path = 'maindirectory/IARCImageBankColpo/Cases_Meta_data_good_v3.csv'\n",
    "try:\n",
    "    metadata = pd.read_csv(metadata_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    metadata = pd.read_csv(metadata_path, encoding='latin1')\n",
    "\n",
    "# Ensure the CaseNumber is formatted to three digits and create a dictionary\n",
    "metadata['CaseNumber'] = metadata['CaseNumber'].apply(lambda x: f'Case {int(x):03d}')\n",
    "diagnosis_dict = pd.Series(metadata['CaseDiagnosis'].values, index=metadata['CaseNumber']).to_dict()\n",
    "\n",
    "# Print the diagnosis dictionary for verification\n",
    "print(\"Diagnosis Dictionary Sample:\", {k: diagnosis_dict[k] for k in list(diagnosis_dict)[:10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d536834-7c0b-4ceb-8218-d1ef27412359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = image / 255.0\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b6729-450b-4c50-a8a5-7efd905c398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_and_preprocess_images(base_path, total_cases=199):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    missing_cases = {'Case 078', 'Case 079'}\n",
    "    \n",
    "    for i in range(1, total_cases + 1):\n",
    "        case_folder = f\"Case {str(i).zfill(3)}\"\n",
    "        if case_folder in missing_cases:\n",
    "            continue\n",
    "        \n",
    "        full_path = os.path.join(base_path, case_folder)\n",
    "        if os.path.exists(full_path):\n",
    "            for filename in os.listdir(full_path):\n",
    "                if filename.lower().endswith('.jpg'):\n",
    "                    file_path = os.path.join(full_path, filename)\n",
    "                    image = preprocess_image(file_path)\n",
    "                    if image is not None:\n",
    "                        all_images.append(image)\n",
    "                        all_labels.append(case_folder)\n",
    "    \n",
    "    return np.array(all_images), np.array(all_labels)\n",
    "\n",
    "# Load images and labels\n",
    "base_path = 'directory/IARCImageBankColpo'\n",
    "all_images, all_labels = load_and_preprocess_images(base_path)\n",
    "\n",
    "print(f\"Preprocessed images: {len(all_images)}\")\n",
    "print(f\"Sample labels: {all_labels[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97445578-16d1-4346-a834-3336d2f7e786",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras or PyTorch, which framework is more appropriate\n",
    "Both TensorFlow/Keras and PyTorch are powerful frameworks for deep learning, but they have different strengths and community support, which might influence your choice depending on your specific needs and preferences. Below is a comparison of the two frameworks in the context of using the Swin-Transformer CNN model:\n",
    "\n",
    "### TensorFlow/Keras\n",
    "\n",
    "#### Pros:\n",
    "1. **High-level API**: Keras, which is integrated into TensorFlow, provides a high-level API that makes building and experimenting with models very user-friendly and concise.\n",
    "2. **Deployment**: TensorFlow has robust support for deployment, including TensorFlow Serving, TensorFlow Lite for mobile and embedded devices, and TensorFlow.js for running models in the browser.\n",
    "3. **Ecosystem**: TensorFlow has a comprehensive ecosystem with tools like TensorBoard for visualization, TensorFlow Extended (TFX) for production ML pipelines, and TensorFlow Hub for pre-trained models.\n",
    "4. **Community and Support**: TensorFlow has a large community and extensive documentation, which can be helpful for troubleshooting and finding resources.\n",
    "\n",
    "#### Cons:\n",
    "1. **Complexity**: TensorFlow can be complex and have a steeper learning curve for beginners compared to PyTorch.\n",
    "2. **Debugging**: TensorFlow's static computational graph can make debugging more challenging compared to PyTorch's dynamic computational graph.\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "#### Pros:\n",
    "1. **Dynamic Computational Graph**: PyTorch's dynamic computational graph (define-by-run) makes it more intuitive and easier to debug.\n",
    "2. **Flexibility**: PyTorch is often praised for its flexibility, making it a preferred choice for research and experimentation.\n",
    "3. **Community and Growth**: PyTorch has seen rapid growth in its user base, especially in the research community, and has a strong community and support network.\n",
    "4. **Integration with Python**: PyTorch integrates seamlessly with the Python ecosystem and libraries, which can make development smoother.\n",
    "\n",
    "#### Cons:\n",
    "1. **Deployment**: While PyTorch has made significant strides with tools like TorchServe, its deployment ecosystem is not as mature as TensorFlow's.\n",
    "2. **API Stability**: PyTorch is relatively younger, and there may be more frequent API changes.\n",
    "\n",
    "### Swin-Transformer CNN Framework\n",
    "\n",
    "The Swin-Transformer is a relatively new model that has gained popularity for its effectiveness in vision tasks. The choice of framework might depend on:\n",
    "\n",
    "- **Pre-trained Models**: Availability of pre-trained Swin-Transformer models in the framework. PyTorch often has cutting-edge models implemented and shared quickly by the research community.\n",
    "- **Ease of Use**: If you prefer a high-level API and ease of model deployment, TensorFlow/Keras might be better. For more flexibility and ease of debugging, PyTorch is advantageous.\n",
    "- **Existing Codebase and Expertise**: Consider the existing codebase and expertise. If you or your team are already proficient in one of the frameworks, it might be beneficial to continue using it.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "For **research and experimentation**, **PyTorch** is often preferred due to its flexibility, dynamic graph, and strong research community support. If you are looking for **robust deployment** and a **high-level API**, **TensorFlow/Keras** might be the better choice.\n",
    "\n",
    "In the context of the Swin-Transformer model, PyTorch might have an edge due to its quick adoption by the research community and availability of pre-trained models. However, both frameworks can be effectively used to implement and train the Swin-Transformer CNN model.\n",
    "\n",
    "**Recommendation**: If you are leaning towards flexibility and cutting-edge research, go with PyTorch. If your focus is on deployment and using a high-level API, consider TensorFlow/Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24cb0f-5ac3-452c-b403-445893fe11af",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "The SWIN-Transformer CNN hybrid model, which combines the strengths of Convolutional Neural Networks (CNNs) and Swin Transformers, typically requires a specific set of preprocessing steps to ensure the input data is in the optimal format for training and inference. Here are the common preprocessing steps required:\n",
    "1. Resizing:\n",
    "\n",
    "The images need to be resized to a fixed size that is suitable for the model. For Swin Transformers, a common input size is 224x224 pixels, but this can vary depending on the specific architecture or any custom modifications.\n",
    "\n",
    "2. Normalization:\n",
    "\n",
    "Pixel values need to be normalized to a specific range, typically [0, 1] or [-1, 1]. Normalization helps in speeding up the convergence of the model during training.\n",
    "\n",
    "3. Data Augmentation:\n",
    "\n",
    "Data augmentation techniques are often applied to increase the diversity of the training dataset and improve the model’s robustness. Common augmentation techniques include:\n",
    "\n",
    "    Random cropping and padding\n",
    "    Random horizontal flipping\n",
    "    Random rotation\n",
    "    Color jittering (adjusting brightness, contrast, saturation, and hue)\n",
    "    Random resizing and scaling\n",
    "\n",
    "4. Converting Images to Tensors:\n",
    "\n",
    "The images need to be converted to tensor format, which is required for input into deep learning models in frameworks like PyTorch or TensorFlow.\n",
    "\n",
    "5. Batching and Shuffling:\n",
    "\n",
    "During training, images should be batched together and shuffled to ensure that the model learns from a diverse set of examples in each epoch.\n",
    "\n",
    "6. Handling Class Imbalance (if any):\n",
    "\n",
    "If the dataset is imbalanced, techniques such as oversampling the minority class or undersampling the majority class can be applied. Additionally, weighted loss functions can be used to handle class imbalance.\n",
    "Example Preprocessing Code for PyTorch:\n",
    "\n",
    "Here is a complete example of how to implement these preprocessing steps using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ecf5e-0c07-4410-b68a-8bdf1fa18e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming all_images, all_labels, and diagnosis_dict are already loaded\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Step 1: Print the initial state\n",
    "print(\"All labels before conversion (sample):\", all_labels[:10])\n",
    "\n",
    "# Step 2: Subset to exclude 'Cancer' and exactly 17 'Normal' cases\n",
    "cancer_cases = {case for case, diagnosis in diagnosis_dict.items() if diagnosis == 'Cancer'}\n",
    "normal_cases = {case for case, diagnosis in diagnosis_dict.items() if diagnosis == 'Normal'}\n",
    "precancerous_cases = {case for case, diagnosis in diagnosis_dict.items() if diagnosis == 'Precancerous'}\n",
    "\n",
    "normal_cases_to_exclude = set(random.sample(normal_cases, 17))  # Randomly select 17 normal cases\n",
    "\n",
    "# Combine cases to exclude\n",
    "cases_to_exclude = cancer_cases.union(normal_cases_to_exclude)\n",
    "print(\"Cases to exclude:\", cases_to_exclude)\n",
    "\n",
    "# Create lists for excluded images and labels\n",
    "excluded_images = []\n",
    "excluded_labels = []\n",
    "remaining_images = []\n",
    "remaining_labels = []\n",
    "\n",
    "# Separate excluded cases from the rest\n",
    "for img, label in zip(all_images, all_labels):\n",
    "    if label in cases_to_exclude:\n",
    "        excluded_images.append(img)\n",
    "        excluded_labels.append(label)\n",
    "    else:\n",
    "        remaining_images.append(img)\n",
    "        remaining_labels.append(label)\n",
    "\n",
    "excluded_images = np.array(excluded_images)\n",
    "excluded_labels = np.array(excluded_labels)\n",
    "remaining_images = np.array(remaining_images)\n",
    "remaining_labels = np.array(remaining_labels)\n",
    "\n",
    "print(f\"Excluded images count: {len(excluded_images)}\")\n",
    "print(f\"Excluded labels count: {len(excluded_labels)}\")\n",
    "print(f\"Remaining images count: {len(remaining_images)}\")\n",
    "print(f\"Remaining labels count: {len(remaining_labels)}\")\n",
    "\n",
    "# Step 3: Randomly pick 54 Normal cases and 54 Precancerous cases for training\n",
    "train_normal_cases = random.sample(list(normal_cases - normal_cases_to_exclude), 54)\n",
    "train_precancerous_cases = random.sample(list(precancerous_cases), 54)\n",
    "\n",
    "# Combine training cases\n",
    "train_cases = set(train_normal_cases + train_precancerous_cases)\n",
    "\n",
    "# Remaining cases for testing\n",
    "test_normal_cases = list((normal_cases - normal_cases_to_exclude) - set(train_normal_cases))\n",
    "test_precancerous_cases = list(precancerous_cases - set(train_precancerous_cases))\n",
    "test_cases = set(test_normal_cases + test_precancerous_cases)\n",
    "\n",
    "# Initialize lists for train and test sets\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "for img, label in zip(remaining_images, remaining_labels):\n",
    "    if label in train_cases:\n",
    "        train_images.append(img)\n",
    "        train_labels.append(label)\n",
    "    elif label in test_cases:\n",
    "        test_images.append(img)\n",
    "        test_labels.append(label)\n",
    "\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(f\"Train images count: {len(train_images)}\")\n",
    "print(f\"Test images count: {len(test_images)}\")\n",
    "print(f\"Train labels count: {len(train_labels)}\")\n",
    "print(f\"Test labels count: {len(test_labels)}\")\n",
    "\n",
    "# Randomly pick 9 Normal cases from the test group and add them to the excluded group\n",
    "additional_excluded_normal_cases = set(random.sample(test_normal_cases, 9))\n",
    "\n",
    "# Create new lists for the final excluded set\n",
    "final_excluded_images = []\n",
    "final_excluded_labels = []\n",
    "\n",
    "for img, label in zip(test_images, test_labels):\n",
    "    if label in additional_excluded_normal_cases:\n",
    "        final_excluded_images.append(img)\n",
    "        final_excluded_labels.append(label)\n",
    "\n",
    "# Combine with the original excluded cases\n",
    "final_excluded_images.extend(excluded_images)\n",
    "final_excluded_labels.extend(excluded_labels)\n",
    "\n",
    "final_excluded_images = np.array(final_excluded_images)\n",
    "final_excluded_labels = np.array(final_excluded_labels)\n",
    "\n",
    "print(f\"Final excluded images count: {len(final_excluded_images)}\")\n",
    "print(f\"Final excluded labels count: {len(final_excluded_labels)}\")\n",
    "\n",
    "# Step 4: Convert the Case numbers in the train, test, and excluded sets to Diagnostic labels using `diagnosis_dict`\n",
    "train_labels_diagnostic = np.array([diagnosis_dict[label] for label in train_labels])\n",
    "test_labels_diagnostic = np.array([diagnosis_dict[label] for label in test_labels])\n",
    "final_excluded_labels_diagnostic = np.array([diagnosis_dict[case] for case in final_excluded_labels])\n",
    "\n",
    "print(\"Train labels (sample):\", train_labels_diagnostic[:10])\n",
    "print(\"Test labels (sample):\", test_labels_diagnostic[:10])\n",
    "print(\"Final excluded labels (sample):\", final_excluded_labels_diagnostic[:10])\n",
    "\n",
    "# Final output for verification\n",
    "print(f\"Total train images: {len(train_images)}\")\n",
    "print(f\"Total test images: {len(test_images)}\")\n",
    "print(f\"Total final excluded images: {len(final_excluded_images)}\")\n",
    "\n",
    "# Step 5: Verify the number of cases for each diagnostic label in the train, test, and excluded groups\n",
    "def count_cases(case_labels, diagnosis_dict):\n",
    "    unique_cases = set(case_labels)\n",
    "    count_normal = sum(1 for case in unique_cases if diagnosis_dict[case] == 'Normal')\n",
    "    count_precancerous = sum(1 for case in unique_cases if diagnosis_dict[case] == 'Precancerous')\n",
    "    count_cancer = sum(1 for case in unique_cases if diagnosis_dict[case] == 'Cancer')\n",
    "    return {'Normal': count_normal, 'Precancerous': count_precancerous, 'Cancer': count_cancer}\n",
    "\n",
    "# Use original case labels for counting unique cases\n",
    "train_case_counts = count_cases(train_labels, diagnosis_dict)\n",
    "test_case_counts = count_cases(test_labels, diagnosis_dict)\n",
    "final_excluded_case_counts = count_cases(final_excluded_labels, diagnosis_dict)\n",
    "\n",
    "print(\"Train set case counts:\", train_case_counts)\n",
    "print(\"Test set case counts:\", test_case_counts)\n",
    "print(\"Final excluded set case counts:\", final_excluded_case_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19502155-4373-4deb-9e8b-a1a3ce9ee2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(train_labels_diagnostic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12af339-daa6-4dcc-9222-01bfd330a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values to check the label format\n",
    "print(\"Unique values in train_labels_diagnostic:\", np.unique(train_labels_diagnostic))\n",
    "print(\"Unique values in test_labels_diagnostic:\", np.unique(test_labels_diagnostic))\n",
    "print(\"Unique values in final_excluded_labels_diagnostic:\", np.unique(final_excluded_labels_diagnostic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95fa53-5251-46d0-ab09-44b365a17c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to display a grid of images\n",
    "def plot_images(images, titles, rows=3, cols=3):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 10))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i])\n",
    "            ax.set_title(f'Label: {titles[i]}')\n",
    "            ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Sample images and their corresponding labels\n",
    "sample_images_n = train_images[1:4]\n",
    "sample_labels_n = train_labels_diagnostic[1:4]\n",
    "sample_images_p = train_images[156:159]\n",
    "sample_labels_p = train_labels_diagnostic[156:159]\n",
    "sample_images_c = final_excluded_images[98:101]\n",
    "sample_labels_c = final_excluded_labels_diagnostic[98:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d10a7-f771-4793-b841-55ae824590ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(sample_images_n, sample_labels_n)\n",
    "plot_images(sample_images_n, sample_labels_p)\n",
    "plot_images(sample_images_n, sample_labels_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de77d5a-f215-4fac-87f1-dabbc01ea651",
   "metadata": {},
   "source": [
    "# SWIN-Transformer CNN hybrid setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dbf75-de66-46bb-bd95-35c6f949915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import timm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from torchvision import transforms\n",
    "\n",
    "# Ensure the labels are in integer format\n",
    "train_labels_diagnostic = np.array(train_labels_diagnostic, dtype=int)\n",
    "test_labels_diagnostic = np.array(test_labels_diagnostic, dtype=int)\n",
    "final_excluded_labels_diagnostic = np.array(final_excluded_labels_diagnostic, dtype=int)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_images = torch.tensor(train_images).permute(0, 3, 1, 2).float()\n",
    "test_images = torch.tensor(test_images).permute(0, 3, 1, 2).float()\n",
    "final_excluded_images = torch.tensor(final_excluded_images).permute(0, 3, 1, 2).float()\n",
    "train_labels_diagnostic = torch.tensor(train_labels_diagnostic, dtype=torch.long)\n",
    "test_labels_diagnostic = torch.tensor(test_labels_diagnostic, dtype=torch.long)\n",
    "final_excluded_labels_diagnostic = torch.tensor(final_excluded_labels_diagnostic, dtype=torch.long)\n",
    "\n",
    "# Custom transformation function\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),  # Ensure the images are 224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.tensors[0][index], self.tensors[1][index]\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert to HWC format for PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors[0])\n",
    "\n",
    "# Create DataLoader for train and test sets\n",
    "train_dataset = CustomTensorDataset((train_images, train_labels_diagnostic), transform=transform)\n",
    "test_dataset = TensorDataset(test_images, test_labels_diagnostic)\n",
    "final_excluded_dataset = TensorDataset(final_excluded_images, final_excluded_labels_diagnostic)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "final_excluded_loader = DataLoader(final_excluded_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.swin_transformer = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=0)\n",
    "        self.fc1 = nn.Linear(1024, 256)  # Adjust the input dimension to match the output of Swin Transformer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1)  # Binary classification (1 output unit)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swin_transformer.forward_features(x)\n",
    "        x = x.mean(dim=[1, 2])  # Global average pooling\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = HybridModel()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=5e-2)  # Using AdamW optimizer with weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)  # Learning rate scheduler\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions)\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "# Function to train the model and evaluate it on the test set at each epoch\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=100):\n",
    "    best_f1 = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_labels, test_predictions = evaluate(model, test_loader)\n",
    "        test_accuracy = accuracy_score(test_labels, test_predictions.round())\n",
    "        test_f1 = f1_score(test_labels, test_predictions.round())\n",
    "        test_auc = roc_auc_score(test_labels, test_predictions)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader.dataset):.4f}, Accuracy: {test_accuracy:.4f}, F1 Score: {test_f1:.4f}, AUC: {test_auc:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if test_f1 > best_f1:\n",
    "            best_f1 = test_f1\n",
    "            best_model_wts = model.state_dict()\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Train the model and evaluate on the test set at each epoch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ac39c-5854-4d7e-829f-4e7abe7b0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "# Evaluate on final excluded set\n",
    "test_labels, test_predictions = evaluate(model, test_loader)\n",
    "# Evaluate on final excluded set\n",
    "final_excluded_labels, final_excluded_predictions = evaluate(model, final_excluded_loader)\n",
    "# Plot ROC Curve for final excluded set\n",
    "fpr, tpr, _ = roc_curve(test_labels, test_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (Test Set)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "# Plot ROC Curve for final excluded set\n",
    "fpr, tpr, _ = roc_curve(final_excluded_labels, final_excluded_predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (Final Excluded Set)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9ed46-fdf8-401b-8c06-cb86788d394f",
   "metadata": {},
   "source": [
    "# Improved version of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44567bda-e56a-4479-ac86-41fee8df2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomTensorDataset(Dataset):\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.tensors[0][index], self.tensors[1][index]\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert to HWC format for PIL\n",
    "        img = Image.fromarray((img * 255).astype(np.uint8))  # Convert to PIL Image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors[0])\n",
    "\n",
    "# Function to prepare DataLoader\n",
    "def prepare_dataloader(images, labels, batch_size, transform):\n",
    "    dataset = CustomTensorDataset(tensors=(images, labels), transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# Image normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Assuming `train_images`, `train_labels`, `test_images`, `test_labels` are already defined\n",
    "train_loader = prepare_dataloader(train_images, train_labels, batch_size=32, transform=transform)\n",
    "test_loader = prepare_dataloader(test_images, test_labels, batch_size=32, transform=transform)\n",
    "\n",
    "# Model Definition\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.swin_transformer = SwinTransformer(img_size=224, patch_size=4, in_chans=3, num_classes=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(49 * 768, 256)  # Adjust based on SwinTransformer output\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swin_transformer.forward_features(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Training the Model\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs):\n",
    "    model.to(device)\n",
    "    best_auc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        scheduler.step(running_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "                outputs = model(images)\n",
    "                predictions = torch.sigmoid(outputs)\n",
    "\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "        auc = roc_auc_score(all_labels, all_predictions)\n",
    "        accuracy = accuracy_score(all_labels, np.round(all_predictions))\n",
    "        f1 = f1_score(all_labels, np.round(all_predictions))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader.dataset):.4f}, \"\n",
    "              f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# Parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HybridModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=5e-2)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84ce5e-d6f0-481a-8ad5-d255be7d6781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the test set at each epoch\n",
    "train(model, train_loader, test_loader, criterion, optimizer, scheduler, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570ef2f-803c-4c37-b721-cd6dc83a916f",
   "metadata": {},
   "source": [
    "# Saving intermediates as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f789f6e-6125-4595-8428-130c4e525994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_images_as_csv(images, file_name):\n",
    "    # Flatten each image to a single row\n",
    "    images_flat = images.reshape(images.shape[0], -1)\n",
    "    # Create a DataFrame\n",
    "    columns = [f'pixel_{i}' for i in range(images_flat.shape[1])]\n",
    "    df = pd.DataFrame(images_flat, columns=columns)\n",
    "    # Save to CSV\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "def save_labels_as_csv(labels, file_name):\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(labels, columns=['label'])\n",
    "    # Save to CSV\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Assuming train_images and train_labels are already defined\n",
    "save_images_as_csv(train_images, 'train_images.csv')\n",
    "save_labels_as_csv(train_labels_diagnostic, 'train_labels.csv')\n",
    "\n",
    "save_images_as_csv(test_images, 'test_images.csv')\n",
    "save_labels_as_csv(test_labels_diagnostic, 'test_labels.csv')\n",
    "\n",
    "save_images_as_csv(final_excluded_images, 'excluded_images.csv')\n",
    "save_labels_as_csv(final_excluded_labels_diagnostic, 'excluded_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4210fb9c-5021-437b-b244-df659138aaee",
   "metadata": {},
   "source": [
    "# Working code for SWIN-Transformer for global context and CNN for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0835b-df48-4dad-a9b3-43cdec76be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, roc_curve, precision_score, recall_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "gamma = 0.8\n",
    "num_folds = 5\n",
    "num_features_to_select = 100  # Adjust this based on the number of features you want to select\n",
    "\n",
    "# Data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Convert labels to numeric format\n",
    "label_to_index_train_test = {'Normal': 0, 'Precancerous': 1}\n",
    "label_to_index_excluded = {'Normal': 0, 'Cancer': 1}\n",
    "train_labels = np.array([label_to_index_train_test[label] for label in train_labels_diagnostic])\n",
    "test_labels = np.array([label_to_index_train_test[label] for label in test_labels_diagnostic])\n",
    "excluded_labels = np.array([label_to_index_excluded[label] for label in final_excluded_labels_diagnostic])\n",
    "\n",
    "# Create TensorDataset\n",
    "tensor_x_train = torch.tensor(train_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "\n",
    "tensor_x_test = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "tensor_x_excluded = torch.tensor(final_excluded_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_excluded = torch.tensor(excluded_labels, dtype=torch.long)\n",
    "excluded_dataset = TensorDataset(tensor_x_excluded, tensor_y_excluded)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.model = model\n",
    "        self.features = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward_features(x)\n",
    "        self.features = x\n",
    "        x = self.model.head(x)\n",
    "        return x\n",
    "\n",
    "def create_model():\n",
    "    model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=2)\n",
    "    return FeatureExtractor(model).to(device)\n",
    "\n",
    "def get_optimizer_scheduler(model, learning_rate, weight_decay):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=gamma)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def extract_features(model, data_loader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "            features.append(model.features.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "def select_top_features(features, labels, num_features):\n",
    "    selector = SelectKBest(score_func=f_classif, k=num_features)\n",
    "    selector.fit(features, labels)\n",
    "    selected_features = selector.transform(features)\n",
    "    return selected_features, selector\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)[:, 1]\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return accuracy, f1, auc, all_labels, all_preds, all_probs\n",
    "\n",
    "def plot_roc_curve(labels, probs, title):\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, title, classes):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves function\n",
    "def plot_learning_curves(train_losses, val_losses, test_accuracies, val_accuracies, title):\n",
    "    # Use the number of epochs instead of a hardcoded 20\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{title} Loss over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title} Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_confusion_matrix_metrics(labels, preds):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    \n",
    "    return accuracy, precision, recall, specificity, f1, auc\n",
    "\n",
    "# Adjusting the training loop to log losses and accuracies per epoch\n",
    "def train_model_with_cv(dataset, num_epochs, num_folds, learning_rate, weight_decay, batch_size):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_excl_accuracy = 0.0\n",
    "    best_accuracy = 0.0\n",
    "    test_accuracies = []\n",
    "    test_f1_scores = []\n",
    "    test_aucs = []\n",
    "    excluded_accuracies = []\n",
    "    excluded_f1_scores = []\n",
    "    excluded_aucs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_indices_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{num_folds}')\n",
    "        val_indices_list.append(val_idx)\n",
    "        train_subsampler = Subset(dataset, train_idx)\n",
    "        val_subsampler = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        excluded_loader = DataLoader(excluded_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = create_model()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer, scheduler = get_optimizer_scheduler(model, learning_rate, weight_decay)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            # Save training loss for plotting\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_accuracy, val_f1, val_auc, val_labels, val_preds, val_probs = evaluate_model(model, val_loader)\n",
    "            val_losses.append(running_loss / len(val_loader))\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "            # Evaluate on test and excluded datasets\n",
    "            test_accuracy, test_f1, test_auc, _, _, _ = evaluate_model(model, test_loader)\n",
    "            excluded_accuracy, excluded_f1, excluded_auc, _, _, _ = evaluate_model(model, excluded_loader)\n",
    "\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_f1_scores.append(test_f1)\n",
    "            test_aucs.append(test_auc)\n",
    "            excluded_accuracies.append(excluded_accuracy)\n",
    "            excluded_f1_scores.append(excluded_f1)\n",
    "            excluded_aucs.append(excluded_auc)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Excluded Accuracy: {excluded_accuracy:.4f}, Excluded F1 Score: {excluded_f1:.4f}, Excluded AUC: {excluded_auc:.4f}\")\n",
    "\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(model.state_dict(), 'best_model_v15.pth')\n",
    "                print(f'Best model saved at epoch {epoch+1} with Test Accuracy: {best_accuracy:.2f}%')\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), 'val_best_model_v15.pth')\n",
    "                print(f'Best val model saved at epoch {epoch+1} with Validation Accuracy: {best_val_accuracy:.2f}%')\n",
    "            if excluded_accuracy > best_excl_accuracy:\n",
    "                best_excl_accuracy = excluded_accuracy\n",
    "                torch.save(model.state_dict(), 'excl_best_model_v15.pth')\n",
    "                print(f'Best excl model saved at epoch {epoch+1} with Excluded Accuracy: {best_excl_accuracy:.2f}%')\n",
    "\n",
    "    return best_accuracy, best_val_accuracy, best_excl_accuracy, train_losses, val_losses, test_accuracies, test_f1_scores, test_aucs, excluded_accuracies, excluded_f1_scores, excluded_aucs, val_indices_list\n",
    "\n",
    "# Hyperparameters tuning\n",
    "learning_rates = [5e-5]\n",
    "weight_decays = [5e-2]\n",
    "batch_sizes = [32]\n",
    "best_params = {}\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for bs in batch_sizes:\n",
    "            print(f'Testing with lr={lr}, wd={wd}, batch_size={bs}')\n",
    "            accuracy, val_accuracy, excl_accuracy, train_losses, val_losses, test_accuracies, test_f1_scores, test_aucs, excluded_accuracies, excluded_f1_scores, excluded_aucs, val_indices_list = train_model_with_cv(train_dataset, num_epochs, num_folds, lr, wd, bs)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'learning_rate': lr, 'weight_decay': wd, 'batch_size': bs}\n",
    "\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Final evaluation on validation, test and excluded datasets\n",
    "best_model = create_model()\n",
    "best_model.load_state_dict(torch.load('best_model_v15.pth'))\n",
    "\n",
    "excl_best_model = create_model()\n",
    "excl_best_model.load_state_dict(torch.load('excl_best_model_v15.pth'))\n",
    "\n",
    "val_best_model = create_model()\n",
    "val_best_model.load_state_dict(torch.load('val_best_model_v15.pth'))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "excluded_loader = DataLoader(excluded_dataset, batch_size=batch_size)\n",
    "\n",
    "# Use the first set of validation indices (from the first fold)\n",
    "val_loader = DataLoader(Subset(train_dataset, val_indices_list[0]), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate models\n",
    "test_accuracy, test_f1, test_auc, test_labels, test_preds, test_probs = evaluate_model(best_model, test_loader)\n",
    "excluded_accuracy, excluded_f1, excluded_auc, excluded_labels, excluded_preds, excluded_probs = evaluate_model(excl_best_model, excluded_loader)\n",
    "val_accuracy, val_f1, val_auc, val_labels, val_preds, val_probs = evaluate_model(val_best_model, val_loader)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plot_roc_curve(val_labels, val_probs, \"ROC Curve for Validation Set (Precancerous vs Normal)\")\n",
    "plot_roc_curve(test_labels, test_probs, \"ROC Curve for Test Set 1 (Precancerous vs Normal)\")\n",
    "plot_roc_curve(excluded_labels, excluded_probs, \"ROC Curve for Test Set 2 (Cancer vs Normal)\")\n",
    "\n",
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(val_labels, val_preds, \"Confusion Matrix for Validation Set (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "plot_confusion_matrix(test_labels, test_preds, \"Confusion Matrix for Test Set 1 (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "plot_confusion_matrix(excluded_labels, excluded_preds, \"Confusion Matrix for Excluded Set 2 (Cancer vs Normal)\", ['Normal', 'Cancer'])\n",
    "\n",
    "# Plot learning curves\n",
    "#plot_learning_curves(train_losses, val_losses, test_accuracies, val_accuracies, \"Training and Validation Loss and Accuracy\")\n",
    "\n",
    "# Calculate confusion matrix metrics\n",
    "val_metrics = calculate_confusion_matrix_metrics(val_labels, val_preds)\n",
    "test_metrics = calculate_confusion_matrix_metrics(test_labels, test_preds)\n",
    "excluded_metrics = calculate_confusion_matrix_metrics(excluded_labels, excluded_preds)\n",
    "\n",
    "print(f'Validation Metrics: Accuracy: {val_metrics[0]:.4f}, Precision: {val_metrics[1]:.4f}, Recall: {val_metrics[2]:.4f}, Specificity: {val_metrics[3]:.4f}, F1 Score: {val_metrics[4]:.4f}, AUC: {val_metrics[5]:.4f}')\n",
    "print(f'Test Metrics: Accuracy: {test_metrics[0]:.4f}, Precision: {test_metrics[1]:.4f}, Recall: {test_metrics[2]:.4f}, Specificity: {test_metrics[3]:.4f}, F1 Score: {test_metrics[4]:.4f}, AUC: {test_metrics[5]:.4f}')\n",
    "print(f'Excluded Metrics: Accuracy: {excluded_metrics[0]:.4f}, Precision: {excluded_metrics[1]:.4f}, Recall: {excluded_metrics[2]:.4f}, Specificity: {excluded_metrics[3]:.4f}, F1 Score: {excluded_metrics[4]:.4f}, AUC: {excluded_metrics[5]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9149a-87c0-4d49-a6da-3258eb8d2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, title, classes):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    # Define a continuous light grey colormap\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'custom_greys', [(0.95, 0.95, 0.95), (0.6, 0.6, 0.6)], N=256)\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes, cbar_kws={'shrink': 0.75})\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcbe616-2515-439c-957a-55d8a7767b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrices\n",
    "plot_confusion_matrix(val_labels, val_preds, \"Confusion Matrix for Validation Set (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "plot_confusion_matrix(test_labels, test_preds, \"Confusion Matrix for Test Set 1 (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "plot_confusion_matrix(excluded_labels, excluded_preds, \"Confusion Matrix for Excluded Set 2 (Cancer vs Normal)\", ['Normal', 'Cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589ec31-67b1-437f-8726-426713ac6234",
   "metadata": {},
   "source": [
    "# This include the hyperparameters grid selection part, and using the extracted features as an imput for for logistic or xgboost for further improvement of the performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f445cd3-de76-4a83-a464-af97f7c2e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import timm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 30\n",
    "gamma = 0.8\n",
    "num_folds = 5\n",
    "num_features_to_select = 100  # Adjust this based on the number of features you want to select\n",
    "\n",
    "# Data augmentation and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Convert labels to numeric format\n",
    "label_to_index_train_test = {'Normal': 0, 'Precancerous': 1}\n",
    "label_to_index_excluded = {'Normal': 0, 'Cancer': 1}\n",
    "train_labels = np.array([label_to_index_train_test[label] for label in train_labels_diagnostic])\n",
    "test_labels = np.array([label_to_index_train_test[label] for label in test_labels_diagnostic])\n",
    "excluded_labels = np.array([label_to_index_excluded[label] for label in final_excluded_labels_diagnostic])\n",
    "\n",
    "# Create TensorDataset\n",
    "tensor_x_train = torch.tensor(train_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "\n",
    "tensor_x_test = torch.tensor(test_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "test_dataset = TensorDataset(tensor_x_test, tensor_y_test)\n",
    "\n",
    "tensor_x_excluded = torch.tensor(final_excluded_images, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "tensor_y_excluded = torch.tensor(excluded_labels, dtype=torch.long)\n",
    "excluded_dataset = TensorDataset(tensor_x_excluded, tensor_y_excluded)\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.model = model\n",
    "        self.features = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model.forward_features(x)\n",
    "        self.features = x\n",
    "        x = self.model.head(x)\n",
    "        return x\n",
    "\n",
    "def create_model():\n",
    "    model = timm.create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=2)\n",
    "    return FeatureExtractor(model).to(device)\n",
    "\n",
    "def get_optimizer_scheduler(model, learning_rate, weight_decay):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=gamma)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def extract_features(model, data_loader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            _ = model(images)\n",
    "            features.append(model.features.cpu().numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "def select_top_features(features, labels, num_features):\n",
    "    selector = SelectKBest(score_func=f_classif, k=num_features)\n",
    "    selector.fit(features, labels)\n",
    "    selected_features = selector.transform(features)\n",
    "    return selected_features, selector\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probs = nn.functional.softmax(outputs, dim=1)[:, 1]\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return accuracy, f1, auc, all_labels, all_probs\n",
    "\n",
    "def plot_roc_curve(labels, probs, title):\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, title, classes):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curves(accuracies, f1_scores, aucs, title):\n",
    "    epochs = list(range(1, num_epochs + 1))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title} Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{title} F1 Score over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, aucs, label='AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'{title} AUC over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model_with_cv(dataset, num_epochs, num_folds, learning_rate, weight_decay, batch_size):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    best_val_accuracy = 0.0\n",
    "    best_excl_accuracy = 0.0\n",
    "    best_accuracy = 0.0\n",
    "    test_accuracies = []\n",
    "    test_f1_scores = []\n",
    "    test_aucs = []\n",
    "    excluded_accuracies = []\n",
    "    excluded_f1_scores = []\n",
    "    excluded_aucs = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{num_folds}')\n",
    "        train_subsampler = Subset(dataset, train_idx)\n",
    "        val_subsampler = Subset(dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        excluded_loader = DataLoader(excluded_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = create_model()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer, scheduler = get_optimizer_scheduler(model, learning_rate, weight_decay)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy, val_f1, val_auc, _, _ = evaluate_model(model, val_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "            # Evaluate on test and excluded datasets\n",
    "            test_accuracy, test_f1, test_auc, _, _ = evaluate_model(model, test_loader)\n",
    "            excluded_accuracy, excluded_f1, excluded_auc, _, _ = evaluate_model(model, excluded_loader)\n",
    "\n",
    "            test_accuracies.append(test_accuracy)\n",
    "            test_f1_scores.append(test_f1)\n",
    "            test_aucs.append(test_auc)\n",
    "            excluded_accuracies.append(excluded_accuracy)\n",
    "            excluded_f1_scores.append(excluded_f1)\n",
    "            excluded_aucs.append(excluded_auc)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}, Test AUC: {test_auc:.4f}\")\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Excluded Accuracy: {excluded_accuracy:.4f}, Excluded F1 Score: {excluded_f1:.4f}, Excluded AUC: {excluded_auc:.4f}\")\n",
    "\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(model.state_dict(), 'best_model_v2.pth')\n",
    "                print(f'Best model saved at epoch {epoch+1} with Test Accuracy: {best_accuracy:.2f}%')\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                torch.save(model.state_dict(), 'val_best_model_v2.pth')\n",
    "                print(f'Best val model saved at epoch {epoch+1} with Validation Accuracy: {best_val_accuracy:.2f}%')\n",
    "            if excluded_accuracy > best_excl_accuracy:\n",
    "                best_excl_accuracy = excluded_accuracy\n",
    "                torch.save(model.state_dict(), 'excl_best_model_v2.pth')\n",
    "                print(f'Best excl model saved at epoch {epoch+1} with Excluded Accuracy: {best_excl_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "    return best_accuracy, best_val_accuracy, best_excl_accuracy, test_accuracies, test_f1_scores, test_aucs, excluded_accuracies, excluded_f1_scores, excluded_aucs\n",
    "\n",
    "# hyperparameter tuning\n",
    "learning_rates =  [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 5e-6]#,\n",
    "weight_decays = [1e-3, 1e-4, 1e-5, 1e-6]\n",
    "batch_sizes = [16, 32, 64]\n",
    "best_params = {}\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for wd in weight_decays:\n",
    "        for bs in batch_sizes:\n",
    "            print(f'Testing with lr={lr}, wd={wd}, batch_size={bs}')\n",
    "            accuracy, test_accuracies, test_f1_scores, test_aucs, excluded_accuracies, excluded_f1_scores, excluded_aucs = train_model_with_cv(train_dataset, num_epochs, num_folds, lr, wd, bs)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'learning_rate': lr, 'weight_decay': wd, 'batch_size': bs}\n",
    "\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Train with the best hyperparameters\n",
    "#batch_size = best_params['batch_size']\n",
    "#train_model_with_cv(train_dataset, num_epochs, num_folds, best_params['learning_rate'], best_params['weight_decay'], batch_size)\n",
    "\n",
    "# Final evaluation on test and excluded datasets\n",
    "best_model = create_model()\n",
    "best_model.load_state_dict(torch.load('best_model_v2.pth'))\n",
    "\n",
    "best_model = create_model()\n",
    "excl_best_model.load_state_dict(torch.load('excl_best_model_v2.pth'))\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "excluded_loader = DataLoader(excluded_dataset, batch_size=batch_size)\n",
    "\n",
    "test_accuracy, test_f1, test_auc, test_labels, test_probs = evaluate_model(best_model, test_loader)\n",
    "excluded_accuracy, excluded_f1, excluded_auc, excluded_labels, excluded_probs = evaluate_model(excl_best_model, excluded_loader)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.2f}')\n",
    "print(f'Test AUC: {test_auc:.2f}')\n",
    "print(f'Excluded Accuracy: {excluded_accuracy:.2f}%')\n",
    "print(f'Excluded F1 Score: {excluded_f1:.2f}')\n",
    "print(f'Excluded AUC: {excluded_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7529e6-cbb7-47bc-8e98-1ab41629f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Ensure you define your dataset\n",
    "# train_dataset = ...\n",
    "\n",
    "# Define the proportion of validation set\n",
    "val_split = 0.2\n",
    "shuffle_dataset = True\n",
    "#random_seed = 42\n",
    "\n",
    "# Create indices for the training and validation splits\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(val_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Create samplers\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(train_dataset, batch_size=32, sampler=val_sampler)\n",
    "\n",
    "# Check the number of samples in each DataLoader\n",
    "print(f'Training samples: {len(train_sampler)}')\n",
    "print(f'Validation samples: {len(val_sampler)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b82c56-2088-417d-8f4e-0a6e41a8f6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test and excluded datasets\n",
    "val_best_model = create_model()\n",
    "val_best_model.load_state_dict(torch.load('val_best_model.pth'))\n",
    "\n",
    "best_model = create_model()\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "excl_best_model = create_model()\n",
    "excl_best_model.load_state_dict(torch.load('excl_best_model.pth'))\n",
    "\n",
    "#val_loader = DataLoader(val_subsampler, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "excluded_loader = DataLoader(excluded_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_accuracy, val_f1, val_auc, val_labels, val_probs = evaluate_model(val_best_model, val_loader)\n",
    "test_accuracy, test_f1, test_auc, test_labels, test_probs = evaluate_model(best_model, test_loader)\n",
    "excluded_accuracy, excluded_f1, excluded_auc, excluded_labels, excluded_probs = evaluate_model(excl_best_model, excluded_loader)\n",
    "\n",
    "print(f'Validation Accuracy: {100 * val_accuracy:.2f}%')\n",
    "print(f'Validation F1 Score: {val_f1:.2f}')\n",
    "print(f'Validation AUC: {val_auc:.2f}')\n",
    "print(f'Test Accuracy: {100 * test_accuracy:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.2f}')\n",
    "print(f'Test AUC: {test_auc:.2f}')\n",
    "print(f'Excluded Accuracy: {100 * excluded_accuracy:.2f}%')\n",
    "print(f'Excluded F1 Score: {excluded_f1:.2f}')\n",
    "print(f'Excluded AUC: {excluded_auc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac6777-c129-4044-ae13-f8aa928a1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "## Extract labels and probabilities from test_metrics_best\n",
    "val_metrics_best = evaluate_model(val_best_model, val_loader)\n",
    "val_labels = val_metrics_best[3]\n",
    "val_probs = val_metrics_best[4]\n",
    "\n",
    "# modified metrics\n",
    "val_metrics_best_actual\n",
    "val_labels_actual = val_metrics_best_actual[3]\n",
    "val_probs_actual = val_metrics_best_actual[4]\n",
    "\n",
    "# Plot ROC curve for the test set with the best model\n",
    "plot_roc_curve(val_labels_actual, val_probs_actual, \"ROC Curve for five-fold cross validation (Precancerous vs Normal)\")\n",
    "\n",
    "\n",
    "# Extract labels and probabilities from test_metrics_best\n",
    "test_metrics_best = evaluate_model(best_model, test_loader)\n",
    "test_labels = test_metrics_best[3]\n",
    "test_probs = test_metrics_best[4]\n",
    "\n",
    "# Plot ROC curve for the test set with the best model\n",
    "plot_roc_curve(test_labels, test_probs, \"ROC Curve for Test Set 1 (Precancerous vs Normal)\")\n",
    "\n",
    "# Evaluate and plot ROC curve for the excluded set with the best model\n",
    "excluded_metrics_best = evaluate_model(excl_best_model, excluded_loader)\n",
    "\n",
    "# Extract labels and probabilities from test_metrics_best\n",
    "excluded_labels = excluded_metrics_best[3]\n",
    "excluded_probs = excluded_metrics_best[4]\n",
    "\n",
    "plot_roc_curve(excluded_labels, excluded_probs, \"ROC Curve for Test Set 2 (Cancer vs Normal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98117ec5-3896-4ac8-a4ff-f7c5353d6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "val_labels, val_predictions = evaluate(val_best_model, val_loader)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_labels, test_predictions = evaluate(best_model, test_loader)\n",
    "\n",
    "# Evaluate on the excluded set set\n",
    "excluded_labels, excluded_predictions = evaluate(excl_best_model, excluded_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc712f3-963c-4ceb-80af-acbd9f46b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            predictions = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions)\n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "# Predict on the validation set\n",
    "val_labels, val_predictions = predict(val_best_model, val_loader)\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "\n",
    "# Predict on the test set\n",
    "test_labels, test_predictions = predict(best_model, test_loader)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "# Predict on the excluded set\n",
    "excluded_labels, excluded_predictions = predict(excl_best_model, final_excluded_loader)\n",
    "excluded_predictions_binary = (excluded_predictions > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7e8ee-ccff-4822-a093-70403af640fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert continuous predictions to binary predictions\n",
    "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
    "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
    "excluded_predictions_binary = (excluded_predictions > 0.5).astype(int)\n",
    "\n",
    "def plot_confusion_matrix(labels, preds, title, classes):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap='Greys')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot Confusion Matrix for the validation set\n",
    "plot_confusion_matrix(val_labels.astype(int), val_predictions_binary, \"Confusion Matrix for five-fold Validation Set (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "\n",
    "# Plot Confusion Matrix for the test set\n",
    "plot_confusion_matrix(test_labels.astype(int), test_predictions_binary, \"Confusion Matrix for Test Set 1 (Precancerous vs Normal)\", ['Normal', 'Precancerous'])\n",
    "\n",
    "# Plot Confusion Matrix for the excluded set\n",
    "plot_confusion_matrix(excluded_labels.astype(int), excluded_predictions_binary, \"Confusion Matrix for Test Set 2 (Cancer vs Normal)\", ['Normal', 'Cancer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddee8c7-8e16-4444-aca8-a3d6c6f1cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_learning_curves(accuracies, f1_scores, aucs, title):\n",
    "    epochs = range(1, len(accuracies) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{title} - F1 Score')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, aucs, label='AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'{title} - AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming test_accuracies, test_f1_scores, test_aucs, excluded_accuracies, excluded_f1_scores, excluded_aucs are defined\n",
    "#plot_learning_curves(val_accuracies, val_f1_scores, val_aucs, \"Validation Set\")\n",
    "plot_learning_curves(test_accuracies, test_f1_scores, test_aucs, \"Test Set\")\n",
    "plot_learning_curves(excluded_accuracies, excluded_f1_scores, excluded_aucs, \"Excluded Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73b961-564b-4235-98c4-ae2746b8a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the features\n",
    "test_features_flat = test_features.reshape(test_features.shape[0], -1)\n",
    "excluded_features_flat = excluded_features.reshape(excluded_features.shape[0], -1)\n",
    "\n",
    "# Select top features using ANOVA F-test\n",
    "selected_test_features, selector = select_top_features(test_features_flat, test_labels, num_features_to_select)\n",
    "selected_excluded_features = selector.transform(excluded_features_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08748839-b04a-4568-b418-d376db56f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression on selected features\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(selected_test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bfec1-72cf-4ca2-b8c4-1a08baa7f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate logistic regression on selected features\n",
    "test_preds = logistic_model.predict(selected_test_features)\n",
    "test_probs = logistic_model.predict_proba(selected_test_features)[:, 1]\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds)\n",
    "test_auc = roc_auc_score(test_labels, test_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569b3f0-e380-401b-a706-451931c229ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_preds = logistic_model.predict(selected_excluded_features)\n",
    "excluded_probs = logistic_model.predict_proba(selected_excluded_features)[:, 1]\n",
    "excluded_accuracy = accuracy_score(excluded_labels, excluded_preds)\n",
    "excluded_f1 = f1_score(excluded_labels, excluded_preds)\n",
    "excluded_auc = roc_auc_score(excluded_labels, excluded_probs)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.2f}')\n",
    "print(f'Test AUC: {test_auc:.2f}')\n",
    "print(f'Excluded Set Accuracy: {excluded_accuracy:.2f}%')\n",
    "print(f'Excluded Set F1 Score: {excluded_f1:.2f}')\n",
    "print(f'Excluded Set AUC: {excluded_auc:.2f}')\n",
    "\n",
    "# Plot ROC curve for test set\n",
    "plot_roc_curve(test_labels, test_probs, \"ROC Curve for Test Set\")\n",
    "\n",
    "# Plot ROC curve for excluded set\n",
    "plot_roc_curve(excluded_labels, excluded_probs, \"ROC Curve for Excluded Set\")\n",
    "\n",
    "# Plot Confusion Matrix for the test set\n",
    "plot_confusion_matrix(test_labels, test_preds, \"Confusion Matrix for Test Set\", ['Normal', 'Precancerous'])\n",
    "\n",
    "# Plot Confusion Matrix for the excluded set\n",
    "plot_confusion_matrix(excluded_labels, excluded_preds, \"Confusion Matrix for Excluded Set\", ['Normal', 'Cancer'])\n",
    "\n",
    "# Plot learning curves\n",
    "plot_learning_curves(test_accuracies, test_f1_scores, test_aucs, \"Test Set\")\n",
    "plot_learning_curves(excluded_accuracies, excluded_f1_scores, excluded_aucs, \"Excluded Set\")\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_learning_curves(accuracies, f1_scores, aucs, title):\n",
    "    epochs = list(range(1, len(accuracies) + 1))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title} Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{title} F1 Score over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, aucs, label='AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'{title} AUC over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for test set\n",
    "plot_learning_curves(test_accuracies, test_f1_scores, test_aucs, \"Test Set\")\n",
    "\n",
    "# Plot learning curves for excluded set\n",
    "plot_learning_curves(excluded_accuracies, excluded_f1_scores, excluded_aucs, \"Excluded Set\")\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_accuracies': test_accuracies,\n",
    "    'test_f1_scores': test_f1_scores,\n",
    "    'test_aucs': test_aucs,\n",
    "    'excluded_accuracies': excluded_accuracies,\n",
    "    'excluded_f1_scores': excluded_f1_scores,\n",
    "    'excluded_aucs': excluded_aucs\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('training_results.csv', index=False)\n",
    "\n",
    "print(\"Results saved to training_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd37da2c-8c8f-4eed-9128-28e5fb010e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have defined train_loader, test_loader, and excluded_loader earlier in the code\n",
    "\n",
    "# Extract features for train, test and excluded datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
    "train_features, train_labels = extract_features(best_model, train_loader)\n",
    "test_features, test_labels = extract_features(best_model, test_loader)\n",
    "excluded_features, excluded_labels = extract_features(best_model, excluded_loader)\n",
    "\n",
    "# Flatten the features\n",
    "train_features_flat = train_features.reshape(train_features.shape[0], -1)\n",
    "test_features_flat = test_features.reshape(test_features.shape[0], -1)\n",
    "excluded_features_flat = excluded_features.reshape(excluded_features.shape[0], -1)\n",
    "\n",
    "# Select top features using ANOVA F-test on the training set\n",
    "selected_train_features, selector = select_top_features(train_features_flat, train_labels, num_features_to_select)\n",
    "selected_test_features = selector.transform(test_features_flat)\n",
    "selected_excluded_features = selector.transform(excluded_features_flat)\n",
    "\n",
    "# Train logistic regression on selected training features\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(selected_train_features, train_labels)\n",
    "\n",
    "# Evaluate logistic regression on selected features for the test set\n",
    "test_preds = logistic_model.predict(selected_test_features)\n",
    "test_probs = logistic_model.predict_proba(selected_test_features)[:, 1]\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds)\n",
    "test_auc = roc_auc_score(test_labels, test_probs)\n",
    "\n",
    "# Evaluate logistic regression on selected features for the excluded set\n",
    "excluded_preds = logistic_model.predict(selected_excluded_features)\n",
    "excluded_probs = logistic_model.predict_proba(selected_excluded_features)[:, 1]\n",
    "excluded_accuracy = accuracy_score(excluded_labels, excluded_preds)\n",
    "excluded_f1 = f1_score(excluded_labels, excluded_preds)\n",
    "excluded_auc = roc_auc_score(excluded_labels, excluded_probs)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1:.2f}')\n",
    "print(f'Test AUC: {test_auc:.2f}')\n",
    "print(f'Excluded Set Accuracy: {excluded_accuracy:.2f}%')\n",
    "print(f'Excluded Set F1 Score: {excluded_f1:.2f}')\n",
    "print(f'Excluded Set AUC: {excluded_auc:.2f}')\n",
    "\n",
    "# Plot ROC curve for test set\n",
    "plot_roc_curve(test_labels, test_probs, \"ROC Curve for Test Set\")\n",
    "\n",
    "# Plot ROC curve for excluded set\n",
    "plot_roc_curve(excluded_labels, excluded_probs, \"ROC Curve for Excluded Set\")\n",
    "\n",
    "# Plot Confusion Matrix for the test set\n",
    "plot_confusion_matrix(test_labels, test_preds, \"Confusion Matrix for Test Set\", ['Normal', 'Precancerous'])\n",
    "\n",
    "# Plot Confusion Matrix for the excluded set\n",
    "plot_confusion_matrix(excluded_labels, excluded_preds, \"Confusion Matrix for Excluded Set\", ['Normal', 'Cancer'])\n",
    "\n",
    "# Plot learning curves\n",
    "def plot_learning_curves(accuracies, f1_scores, aucs, title):\n",
    "    epochs = list(range(1, len(accuracies) + 1))\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs, accuracies, label='Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{title} Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'{title} F1 Score over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, aucs, label='AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title(f'{title} AUC over Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot learning curves for test set\n",
    "plot_learning_curves(test_accuracies, test_f1_scores, test_aucs, \"Test Set\")\n",
    "\n",
    "# Plot learning curves for excluded set\n",
    "plot_learning_curves(excluded_accuracies, excluded_f1_scores, excluded_aucs, \"Excluded Set\")\n",
    "\n",
    "# Save the results\n",
    "results = {\n",
    "    'test_accuracies': test_accuracies,\n",
    "    'test_f1_scores': test_f1_scores,\n",
    "    'test_aucs': test_aucs,\n",
    "    'excluded_accuracies': excluded_accuracies,\n",
    "    'excluded_f1_scores': excluded_f1_scores,\n",
    "    'excluded_aucs': excluded_aucs\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('training_results.csv', index=False)\n",
    "\n",
    "print(\"Results saved to training_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
